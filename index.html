<!-- saved from url=(0027)http://wanglimin.github.io/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiankang Deng</title>
  <meta content="Jiankang Deng, wanglimin.github.io" name="keywords">
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: inherit;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 3em auto 2em auto;
      width: 800px;
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 14px;
      background: #eee;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
    }

    strong {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
    }

    ul {
      list-style: circle;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      margin-bottom: 1.5em;
      background: #eee;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.5em;
      margin-bottom: 1em;
      border: 1px solid #ddd;
      background: #fff;
      padding: 1em 1em 1em 1em;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
      margin: 1em 0;
      padding: 0;
    }

    div.paper pre {
      font-size: 0.9em;
    }
  </style>

  <link href="./resources/css" rel="stylesheet" type="text/css">

  <script async="" src="./resources/analytics.js"></script>
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-45959174-3', 'wanglimin.github.io');
    ga('send', 'pageview');
  </script>
</head>


<body>
  <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
      <img title="jdeng" style="float: left; padding-left: .01em; height: 130px;" src="./resources/jdeng.png">
      <div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Jiankang Deng</span><br>
        <span><a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group</a></span> <br>
        <span>Imperial College London, UK</span><br>
        <span><strong>Office</strong>: 312 Huxley Building, South Kensington Campus, SW7 2AZ</span><br>
        <span><strong>Email</strong>: jiankangdeng [at] gmail.com</span> <br>
        <span><strong>Phone</strong>: +44 (0)77 069 09778 </span> <br>
      </div>
    </div>
  </div>
  <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

  <div style="clear: both;">
    <div class="section">
      <h2>About Me (<a href="http://wanglimin.github.io/wlm_cv.pdf">CV</a>)</h2>
      <div class="paper">
        I am currently a PhD student with <a href="https://wp.doc.ic.ac.uk/szafeiri/">Stefanos Zafeiriou</a> in the <a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group</a> (IBUG) at <a href="https://www.imperial.ac.uk/">Imperial College London</a>. My research topic is visual localization, and I am transferring from engineering thinking to academic view.
        <br> <br> Before coming to IBUG, I obtained my master and bachelor degree from <a href="http://www.nuist.edu.cn/newindex/index.htm"> Nanjing Universify of Information Science and Technology</a>. During my master study, I was with the <a href="http://bdat.nuist.edu.cn/">BDAT Lab</a> under the supervision of Prof. <a href="https://scholar.google.com/citations?user=FgFUwXAAAAAJ&hl=zh-CN">Qingshan Liu</a>, and I worked closely with Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=zh-CN">Dacheng Tao</a>.
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>

					<li> 2017-08-01:
						I am annotating human landmarks for <a href="http://activity-net.org/"> ActivityNet </a>, which I believe is useful for spatial-temporal action detection.
					</li>

          <li> 2017-07-17:
						I have collected 20 yellow bars from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) within last three years. <a href="http://image-net.org/">ImageNet Challenge</a> is a prestigious competition commonly known as the Olympics of computer vision.
          </li>

          <li> 2017-04-10:
            <alert>I am collecting 3D faces from Science Museum.</alert>. For more details, please see the <a href="http://www3.imperial.ac.uk/newsandeventspggrp/imperialcollege/newssummary/news_2-6-2017-10-47-29">introduction page</a>.
          </li>

        </ul>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Recent Publications [ <a href="https://scholar.google.com.hk/citations?user=HEuN8PcAAAAJ&amp;hl=en">Google Scholar</a> ]</h2>

      <div class="paper" id="menpo_CVPRW17"><img class="paper" src="./resources/menpobenchmark.png" title="The Menpo Facial Landmark Localisation Challenge: A step closer to the solution">
        <div> <strong>The Menpo Facial Landmark Localisation Challenge: A step closer to the solution</strong><br> S. Zafeiriou, G. Trigeorgis, G. Chrysos, J. Deng and J. Shen <br> in IEEE Conference on Computer Vision and Pattern Recognition Workshops(<strong>CVPRW</strong>), 2017. <br>
					[ <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Zafeiriou_The_Menpo_Facial_CVPR_2017_paper.pdf">Paper</a> ]
					[ <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:syZXuu3QOnQJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWYDd3Xv4yND5y6Hbtshsp4SA314HiZ89&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTex</a> ]
					[ <a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">Project Page</a>] <br>
          <alert>Multi-view face alignment dataset</alert>
        </div>
        <div class="spanner"></div>
      </div>alert

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Contests</h2>
      <div class="paper">
        <ul>
          <li>
          ActivityNet 2017: Untrimmed Video Classification, <strong>Rank 1st</strong>.
				  </li>
          <li>
					ILSVRC 2017: Object detection with provided/additional training data, <strong>Rank 1st</strong>.
				  </li>
					<li>
					ILSVRC 2017: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2017: Object detection/tracking from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016: Object detection/tracking from video with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2015: Object detection with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2015: Object detection from video with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-VW 2015: Face detection, alignment and tracking from videos, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-W 2014: Face detection and alignment from images, <strong>Rank 1st (Academic)</strong>.
					</li>
        </ul>
        <div class="spanner"></div>
      </div>
    </div>
  </div>

	<div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Academic presentations</h2>
      <div class="paper">

			CVPR 2017, Beyond ImageNet Large Scale Visual Recognition Challenge, <a href="http://image-net.org/challenges/beyond_ilsvrc"> Speed/Accuracy Trade-offs for Object Detection from Video</a> <br>

			CVPR 2017, Beyond ImageNet Large Scale Visual Recognition Challenge, <a href="http://image-net.org/challenges/beyond_ilsvrc"> BDAT Object Detection, </a> <br>

			ECCV 2016, ImageNet and COCO Visual Recognition Challenges, <a href="http://image-net.org/challenges/ilsvrc+coco2016"> Efficient Object Detection From Videos</a> <br>

			ICCV 2015, ImageNet and MS COCO Visual Recognition Challenges, <a href="http://image-net.org/challenges/ilsvrc+mscoco2015">Cascade Region Regression for Robust Object Detection</a>

      <div class="spanner"></div>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>Academic activities</h2>

      <div class="paper">

		  ICCV 2017,
			<a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">1st 3D Face Tracking in-the-wild Competition</a> <br>

			CVPR 2017,
			<a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">2nd Facial Landmark Localisation Competition - The Menpo BenchMark</a>

      </div>

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>Friends</h2>
      <div class="paper">
        <a href="http://shaoqingren.com/">Shaoqing Ren</a> (Momenta)<br>
				<a href="http://www.cbsr.ia.ac.cn/users/jjyan/main.htm">Junjie Yan</a> (SenseTime) <br>
				<a href="http://kaihuazhang.net/">Kaihua Zhang</a> (NUIST) <br>
				<a href="https://sites.google.com/site/xtyuan1980/">Xiaotong Yuan</a> (NUIST) <br>
				<a href="https://sites.google.com/site/guangcanliu/">Guangcan Liu</a> (NUIST) <br>
				<a href="http://www.jifengdai.org/">Jifeng Dai</a> (MSRA) <br>
				<a href="https://www.microsoft.com/en-us/research/people/yichenw/">Yichen Wei</a> (MSRA)
      </div>
    </div>
  </div>

	<div style="clear: both;">
		<div class="section">
			<h2>GPU Donations</h2>
			<div class="paper">
				<alert> One Titan X Donation from ILSVRC 2015 </alert><br>
				<alert> One Titan Xp Donation from ActivityNet 2017</alert><br>
			</div>
		</div>
	</div>

  <div style="clear:both;">
    <p align="right">
      <font size="5">Last Updated on 1th Aug., 2017</font>
    </p>
    <p align="right">
      <font size="5">Published with <a href="https://pages.github.com/">GitHub Pages</a></font>
    </p>
  </div>
</body>
</html>
