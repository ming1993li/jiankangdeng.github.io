<!-- saved from url=(0027)http://wanglimin.github.io/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiankang Deng</title>
  <meta content="Jiankang Deng, wanglimin.github.io" name="keywords">
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: inherit;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 3em auto 2em auto;
      width: 800px;
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 14px;
      background: #eee;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
    }

    strong {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
    }

    ul {
      list-style: circle;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      margin-bottom: 1.5em;
      background: #eee;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.5em;
      margin-bottom: 1em;
      border: 1px solid #ddd;
      background: #fff;
      padding: 1em 1em 1em 1em;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
      margin: 1em 0;
      padding: 0;
    }

    div.paper pre {
      font-size: 0.9em;
    }
  </style>

  <link href="./resources/css" rel="stylesheet" type="text/css">

  <script async="" src="./resources/analytics.js"></script>
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-45959174-3', 'wanglimin.github.io');
    ga('send', 'pageview');
  </script>
</head>


<body>
  <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
      <img title="jdeng" style="float: left; padding-left: .01em; height: 130px;" src="./resources/jdeng.png">
      <div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Jiankang Deng</span><br>
        <span><a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group</a></span> <br>
        <span>Imperial College London, UK</span><br>
        <span><strong>Office</strong>: 312 Huxley Building, South Kensington Campus, SW7 2AZ</span><br>
        <span><strong>Email</strong>: jiankangdeng [at] gmail.com</span> <br>
        <span><strong>Phone</strong>: +44 (0)77 069 09778 </span> <br>
      </div>
    </div>
  </div>
  <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

  <div style="clear: both;">
    <div class="section">
      <h2>About Me (<a href="http://jiankangdeng.github.io">CV</a>)</h2>
      <div class="paper">
        I am currently a PhD student with <a href="https://wp.doc.ic.ac.uk/szafeiri/">Stefanos Zafeiriou</a> in the <a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group</a> (IBUG) at <a href="https://www.imperial.ac.uk/">Imperial College London</a>. I am funded by the <a href="http://www.imperial.ac.uk/study/pg/fees-and-funding/scholarships/presidents-phd-scholarship">Imperial President's PhD Scholarships</a>. I am in the project of <a href="https://facer2vm.org/">EPSRC FACER2VM</a>: Face Matching for Automatic Identity Retrieval, Recognition, Verification and Management. The project is led by <a href="https://www.surrey.ac.uk/cvssp/people/josef_kittler/">Prof. Josef Kittler</a>, and the total grant value is <a href="http://gow.epsrc.ac.uk/NGBOChooseTTS.aspx?Mode=ResearchArea&ItemDesc=Image+and+Vision+Computing">£6,104,265</a>. My research topic is visual localization (face, human, and object), and I am transferring from engineering style (better, faster and smaller）to academic thinking (elegant, rigorous and clear).<br>

        Before coming to IBUG, I obtained my master and bachelor degree from <a href="http://www.nuist.edu.cn/newindex/index.htm"> Nanjing Universify of Information Science and Technology</a>. During my master study, I was with the <a href="http://bdat.nuist.edu.cn/">BDAT Lab</a> under the supervision of Prof. <a href="https://scholar.google.com/citations?user=FgFUwXAAAAAJ&hl=zh-CN">Qingshan Liu</a>, and I worked closely with Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=zh-CN">Dacheng Tao</a>.
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>

					<li> 2017-08-01:
						I am annotating human landmarks for <a href="http://activity-net.org/"> ActivityNet </a>, which I believe is useful for spatial-temporal action detection although the annotation is time-consuming and labor-intensive.
					</li>

          <li> 2017-07-17:
						In last three years, I have collected <alert>20/43 yellow bars</alert> (<a href="http://image-net.org/challenges/LSVRC/2017/results">10 in 2017</a>, <a href="http://image-net.org/challenges/LSVRC/2016/results">5 in 2016</a> and <a href="http://image-net.org/challenges/LSVRC/2015/results">5 in 2015/a>) from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). <a href="http://image-net.org/">ImageNet Challenge</a> is the most prestigious competition commonly known as the Olympics of computer vision.
          </li>

          <li> 2017-04-10:
            I am collecting 3D faces from Science Museum. For more details, please see the <a href="http://www3.imperial.ac.uk/newsandeventspggrp/imperialcollege/newssummary/news_2-6-2017-10-47-29">introduction page</a>.
          </li>

        </ul>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Recent Publications [ <a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Jiankang">DBLP</a> ]</h2>

      <div class="paper" id="menpo_CVPRW17"><img class="paper" src="./resources/menpobenchmark.png" title="The Menpo Facial Landmark Localisation Challenge: A step closer to the solution">
        <div> <strong>The Menpo Facial Landmark Localisation Challenge: A step closer to the solution</strong><br> S. Zafeiriou, G. Trigeorgis, G. Chrysos, J. Deng and J. Shen <br> in IEEE Conference on Computer Vision and Pattern Recognition Workshops(<strong>CVPRW</strong>), 2017. <br>
					[ <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Zafeiriou_The_Menpo_Facial_CVPR_2017_paper.pdf">Paper</a> ]
					[ <a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">Project Page</a>] <br>
          <alert>Multi-view (semi-frontal and profile) face alignment dataset</alert>
        </div>
        <div class="spanner"></div>
      </div>

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Contests</h2>
      <div class="paper">
        <ul>
          <li>
          ActivityNet 2017: Untrimmed Video Classification, <strong>Rank 1st</strong>.
				  </li>
          <li>
					ILSVRC 2017: Object detection with provided/additional training data, <strong>Rank 1st</strong>.
				  </li>
					<li>
					ILSVRC 2017: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2017: Object detection/tracking from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016: Object detection/tracking from video with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2015: Object detection with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2015: Object detection from video with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-VW 2015: Face detection, alignment and tracking from videos, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-W 2014: Face detection and alignment from images, <strong>Rank 1st (Academic)</strong>.
					</li>
        </ul>
        <div class="spanner"></div>
      </div>
    </div>
  </div>

	<div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Academic Presentations</h2>
      <div class="paper">

			CVPR 2017, Beyond ImageNet Large Scale Visual Recognition Challenge, <a href="http://image-net.org/challenges/beyond_ilsvrc"> Speed/Accuracy Trade-offs for Object Detection from Video</a> <br>

			CVPR 2017, Beyond ImageNet Large Scale Visual Recognition Challenge, <a href="http://image-net.org/challenges/beyond_ilsvrc"> BDAT Object Detection </a> <br>

			ECCV 2016, ImageNet and COCO Visual Recognition Challenges, <a href="http://image-net.org/challenges/ilsvrc+coco2016"> Efficient Object Detection From Videos</a> <br>

			ICCV 2015, ImageNet and MS COCO Visual Recognition Challenges, <a href="http://image-net.org/challenges/ilsvrc+mscoco2015"> Cascade Region Regression for Robust Object Detection</a>

      <div class="spanner"></div>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>Competition Organization</h2>

      <div class="paper">

		  ICCV 2017,
			<a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">1st 3D Face Tracking in-the-wild Competition</a> <br>

			CVPR 2017,
			<a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">2nd Facial Landmark Localisation Competition - The Menpo BenchMark</a>

      </div>

    </div>
  </div>

	<div style="clear: both;">
		<div class="section">
			<h2>Intenship</h2>
			<div class="paper">

	    2015.03 - 2015.08: <strong>MSRA</strong>, Clothing Detection and Analysis. Work with <a href="https://scholar.google.co.uk/citations?user=AUhj438AAAAJ&hl=en">Shaoqing Ren</a>, <a href="https://scholar.google.com/citations?user=8qSLKUEAAAAJ&hl=zh-CN">Yichen Wei</a> and <a> href="https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=zh-CN">Jian Sun</a>in Visual Computing Group of Microsoft Research Asia.<br>

	    2013.12 - 2014.05: <strong>Baidu IDL</strong>, Face Detection and Analysis. Work with <a href="https://scholar.google.com/citations?user=rEYarG0AAAAJ&hl=zh-CN">Junjie Yan</a>, <a href="https://scholar.google.com/citations?user=VUeFRoEAAAAJ&hl=zh-CN">Yafeng Deng</a>, and <a href="https://scholar.google.com/citations?user=IyyEKyIAAAAJ&hl=zh-CN">Chang Huang</a> in Baidu Institute of Deep Learning.

			</div>
		</div>
	</div>

  <div style="clear: both;">
    <div class="section">
      <h2>Friends</h2>
      <div class="paper">
        <a href="http://shaoqingren.com/">Shaoqing Ren</a> (Momenta);
				<a href="http://www.cbsr.ia.ac.cn/users/jjyan/main.htm">Junjie Yan</a> (SenseTime);
				<a href="http://peilinzhao.weebly.com/">Peilin Zhao</a> (Ant Financial);
				<a href="http://ffmpbgrnn.github.io/">Linchao Zhu</a> (UTS);
				<a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=zh-CN">Shaoli Huang</a> (UTS);
				<a href="http://mingminggong.xyz/">Mingming Gong</a> (UTS);
				<a href="http://tongliangliu.esy.es/">Tongliang Liu</a> (USYD);
				<a href="https://www.microsoft.com/en-us/research/people/doch/">Dong Chen</a> (MSRA);
				<a href="http://www.jifengdai.org/">Jifeng Dai</a> (MSRA);
				<a href="https://www.microsoft.com/en-us/research/people/yichenw/">Yichen Wei</a> (MSRA);
				<a href="http://kaihuazhang.net/">Kaihua Zhang</a> (NUIST);
				<a href="https://sites.google.com/site/xtyuan1980/">Xiaotong Yuan</a> (NUIST);
				<a href="https://sites.google.com/site/guangcanliu/">Guangcan Liu</a> (NUIST);
				<a href="hhttps://scholar.google.com/citations?user=s_8zWpgAAAAJ&hl=zh-CN">Yi Wu</a> (NAU);
      </div>
    </div>
  </div>

	<div style="clear: both;">
		<div class="section">
			<h2>Computing Resource Support</h2>
			<div class="paper">
				<alert> One NVIDIA GTX TITAN X donation from ILSVRC 2015 </alert><br>
				<alert> Two NVIDIA NVIDIA GeForce GTX 1080 donation from Imperial President's PhD Scholarships 2016 </alert><br>
				<alert> One NVIDIA GTX TITAN Xp donation from ActivityNet 2017 </alert><br>
				<alert> ***Any kind of GPU donation is welcome!*** </alert>
			</div>
		</div>
	</div>

  <div style="clear:both;">
    <p align="right">
      <font size="5">Last Updated on 1th Aug., 2017</font>
    </p>
    <p align="right">
      <font size="5">Published with <a href="https://pages.github.com/">GitHub Pages</a></font>
    </p>
  </div>
</body>
</html>
