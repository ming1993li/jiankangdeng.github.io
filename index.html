<!-- saved from url=(0027)http://wanglimin.github.io/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiankang Deng</title>
  <meta content="Jiankang Deng, wanglimin.github.io" name="keywords">
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: inherit;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 3em auto 2em auto;
      width: 800px;
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 14px;
      background: #eee;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
    }

    strong {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
    }

    ul {
      list-style: circle;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      margin-bottom: 1.5em;
      background: #eee;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.5em;
      margin-bottom: 1em;
      border: 1px solid #ddd;
      background: #fff;
      padding: 1em 1em 1em 1em;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
      margin: 1em 0;
      padding: 0;
    }

    div.paper pre {
      font-size: 0.9em;
    }
  </style>

  <link href="./resources/css" rel="stylesheet" type="text/css">

  <script async="" src="./resources/analytics.js"></script>
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-45959174-3', 'wanglimin.github.io');
    ga('send', 'pageview');
  </script>
</head>


<body>
  <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
      <img title="jdeng" style="float: left; padding-left: .01em; height: 130px;" src="./resources/jdeng.png">
      <div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Jiankang Deng</span><br>
        <span><a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group</a></span> <br>
        <span>Imperial College London, UK</span><br>
        <span><strong>Office</strong>: 312 Huxley Building, South Kensington Campus, SW7 2AZ</span><br>
        <span><strong>Email</strong>: jiankangdeng [at] gmail.com</span> <br>
        <span><strong>Phone</strong>: +44 (0)77 069 09778 </span> <br>
      </div>
    </div>
  </div>
  <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

  <div style="clear: both;">
    <div class="section">
      <h2>About Me (<a href="http://wanglimin.github.io/wlm_cv.pdf">CV</a>)</h2>
      <div class="paper">
        I am currently a PhD student with <a href="https://wp.doc.ic.ac.uk/szafeiri/">Stefanos Zafeiriou</a> in the <a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group</a> (IBUG) at <a href="https://www.imperial.ac.uk/">Imperial College London</a>.
        <br> <br> Before coming to IBUG, I obtained my master and bachelor degree from <a href="http://www.nuist.edu.cn/newindex/index.htm"> Nanjing Universify of Information Science and Technology</a>. During my master study, I was with the <a href="http://bdat.nuist.edu.cn/">BDAT Lab</a> under the supervision of Prof. <a href="https://scholar.google.com/citations?user=FgFUwXAAAAAJ&hl=zh-CN">Qingshan Liu</a>, and I worked closely with Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=zh-CN">Dacheng Tao</a>.
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>
          <li> 2017-07-18: I am invited to give a talk at the <a href="https://sites.google.com/view/fvt2017/home">Workshop on Frontiers of Video Technology-2017</a> [ <a href="http://wanglimin.github.io/fvt_slide.pdf">Slide</a> ].
          </li>
          <li> 2017-03-28:
            <alert>I am co-organizing the CVPR2017 workshop and challenge on Visual Understanding by Learning from Web Data</alert>. For more details, please see the <a href="http://www.vision.ee.ethz.ch/webvision/workshop.html">workshop page</a> and <a href="https://competitions.codalab.org/competitions/16439">challenge page</a>.
            </li>

          <li> 2016-12-20:
            <alert>We release the code and models for SR-CNN paper</alert> [ <a href="https://github.com/yifita/action.sr_cnn">Code</a> ]. </li>
          <li> 2016-10-05:
            <alert>We release the code and models for Places2 scene recognition challenge</alert> [ <a href="https://arxiv.org/abs/1610.01119">arXiv</a> ] [ <a href="https://github.com/wanglimin/MRCNN-Scene-Recognition">Code</a> ]. </li>
          <li> 2016-08-03:
            <alert>Code and model of Temporal Segment Networks is released</alert> [ <a href="http://arxiv.org/abs/1608.00859">arXiv</a> ] [ <a href="https://github.com/yjxiong/temporal-segment-networks">Code</a> ]. </li>

          <li> 2016-06-16:
            <alert>Our team secures the 1st place for untrimmed video classification at ActivityNet Challenge 2016</alert> [ <a href="http://activity-net.org/challenges/2016/program.html">Result</a> ]. <br> Basically, our solution is based on our works of
            <a
              href="https://github.com/yjxiong/temporal-segment-networks">Temporal Segment Networks</a> (TSN) and <a href="https://github.com/wanglimin/TDD">Trajectory-pooled Deep-convolutional Descriptors</a> (TDD). </li>

          <li> 2015-12-10:
            <alert>Our SIAT_MMLAB team secures the 2nd place for scene recognition at ILSVRC 2015 </alert> [ <a href="http://image-net.org/challenges/LSVRC/2015/results#scene">Result</a> ].</li>
          
          <li> 2015-08-07:
            <alert>We release the Places205-VGGNet models</alert> [ <a href="https://github.com/wanglimin/Places205-VGGNet">Link</a> ]. </li>
          <li> 2015-07-22:
            <alert>Code of Trajectory-Pooled Deep-onvolutional Descriptors (TDD) is released</alert> [ <a href="https://github.com/wanglimin/TDD">Link</a> ]. </li>
          <li> 2015-07-15:
            <alert>Very deep two stream ConvNets are proposed for action recognition</alert> [ <a href="http://arxiv.org/abs/1507.02159">Link</a> ]. </li>
          <li> 2015-03-15: We are the 1st winner of both tracks for action recognition and cultural event recognition, on ChaLearn <a href="http://gesture.chalearn.org/">Looking at People Challenge</a> at CVPR 2015. </li>

        </ul>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Recent Publications [ <a href="http://wanglimin.github.io/publication.html">Full List</a> ] [ <a href="https://scholar.google.com.hk/citations?user=HEuN8PcAAAAJ&amp;hl=en">Google Scholar</a> ]</h2>


      <div class="paper" id="WangQTV_CVPR16"><img class="paper" src="./resources/WangQTV_CVPR16.jpg" title="Actionness Estimation Using Hybrid Fully Convolutional Networks">
        <div> <strong>Actionness Estimation Using Hybrid Fully Convolutional Networks</strong><br> L. Wang, Y. Qiao, X. Tang, and L. Van Gool <br> in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. <br> [ <a href="http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf">Paper</a>          ] [ <a href="http://wanglimin.github.io/papers/WangQTV_CVPR16.bib">BibTex</a> ] [ <a href="http://wanglimin.github.io/papers/WangQTV_CVPR16_Poster.pdf">Poster</a> ] [ <a href="http://wanglimin.github.io/actionness_hfcn/index.html">Project Page</a>          ] [ <a href="https://github.com/wanglimin/actionness-estimation/">Code</a> ]<br>
          <alert>Estimating actionness maps and generating action proposals</alert>
        </div>
        <div class="spanner"></div>
      </div>

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Contests</h2>
      <div class="paper">
        <ul>
          <li>
          ActivityNet 2017: Untrimmed Video Classification, <strong>Rank 1st</strong>.
				  </li>
          <li>
					ILSVRC 2017</alert>: Object detection with provided/additional training data, <strong>Rank 1st</strong>.
				  </li>
					<li>
					ILSVRC 2017</alert>: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2017</alert>: Object detection/tracking from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016</alert>: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016</alert>: Object detection/tracking from video with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2015</alert>: Object detection with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2015</alert>: Object detection from video with additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-VW 2015</alert>: Face detection, alignment and tracking from videos, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-W 2014</alert>: Face detection and alignment from images, <strong>Rank 1st (Academic)</strong>.
					</li>
					<li>
					300-W 2014</alert>: Face detection and alignment from images, <strong>Rank 1st (Academic)</strong>.
					</li>

        </ul>
        <div class="spanner"></div>
      </div>
    </div>
  </div>

	<div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Academic presentations</h2>
      <div class="paper">

			<a href="http://image-net.org/challenges/beyond_ilsvrc">BDAT Object Detection, CVPR 2017</a> <br>

			<a href="http://image-net.org/challenges/beyond_ilsvrc">Speed/Accuracy Trade-offs for Object Detection from Video, CVPR 2017</a> <br>

			<a href="http://image-net.org/challenges/ilsvrc+coco2016">Efficient Object Detection From Videos, ECCV 2016</a> <br>

			<a href="http://image-net.org/challenges/ilsvrc+mscoco2015">Cascade Region Regression for Robust Object Detection, ICCV 2015</a>

      <div class="spanner"></div>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Academic activities</h2>
      <div class="paper">

			<a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">1st 3D Face Tracking in-the-wild Competition</a> <br>

			<a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">2nd Facial Landmark Localisation Competition - The Menpo BenchMark</a>

      <div class="spanner"></div>
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>Friends</h2>
      <div class="paper">
        <a href="http://guoshengcv.github.io/">Shaoqing Ren</a> (SIAT), <a href="http://www.whuang.org/">Junjie Yan</a> (Oxford), <a href="http://zbwglory.github.io/">Bowen Zhang</a> (Tongji University), <a href="http://wangzheallen.github.io/">Zhe Wang</a>        (SIAT), <a href="http://liwei.ml/">Wei Li</a> (Google), <a href="http://personal.ie.cuhk.edu.hk/~xy012/">Yuanjun Xiong</a> (CUHK), <a href="http://pengxj.github.io/">Xiaojiang Peng</a> (INRIA), <a href="https://zhuoweic.github.io/">Zhuowei Cai</a>        (Univ. of Wisconsin-Madison), <a href="https://scholar.google.com.sg/citations?user=fPwq28oAAAAJ&amp;hl=en">Xingxing Wang</a> (NTU)
      </div>
    </div>
  </div>

  <div style="clear:both;">
    <p align="right">
      <font size="5">Last Updated on 1th Aug., 2017</font>
    </p>
    <p align="right">
      <font size="5">Published with <a href="https://pages.github.com/">GitHub Pages</a></font>
    </p>
  </div>

</body>

</html>
