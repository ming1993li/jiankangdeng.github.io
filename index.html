<!-- saved from url=(0027)http://wanglimin.github.io/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiankang Deng</title>
  <meta content="Jiankang Deng, jiankangdeng.github.io" name="keywords">
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: inherit;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 3em auto 2em auto;
      width: 800px;
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 14px;
      background: #eee;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
    }

    strong {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
    }

    ul {
      list-style: circle;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: bold;
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      margin-bottom: 1.5em;
      background: #eee;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.5em;
      margin-bottom: 1em;
      border: 1px solid #ddd;
      background: #fff;
      padding: 1em 1em 1em 1em;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
      margin: 1em 0;
      padding: 0;
    }

    div.paper pre {
      font-size: 0.9em;
    }
  </style>

  <link href="./resources/css" rel="stylesheet" type="text/css">

  <script async="" src="./resources/analytics.js"></script>
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-45959174-3', 'jiankangdeng.github.io');
    ga('send', 'pageview');
  </script>
</head>


<body>
  <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
      <img title="jdeng" style="float: left; padding-left: .01em; height: 130px;" src="./resources/jdeng.png">
      <div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Jiankang Deng</span><br>
        <span><a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group (IBUG)</a></span> <br>
        <span>Department of Computing, Imperial College London, UK</span><br>
        <span><strong>Office</strong>: 351 Huxley Building, 180 Queenâ€™s Gate, SW7 2AZ</span><br>
        <span><strong>Email</strong>: jiankangdeng [at] gmail.com ; j.deng16 [at] imperial.ac.uk </span> <br>
        <span><strong>Phone</strong>: +44 (0) 77 0690 9778 </span> <br>
      </div>
    </div>
  </div>
  <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

  <div style="clear: both;">
    <div class="section">
      <h2>About Me (<a href="http://jiankangdeng.github.io/resources/CV_jiankangdeng.pdf">CV</a>)</h2>
      <div class="paper">
        I am a Ph.D. student in the <a href="https://ibug.doc.ic.ac.uk/home">Intelligent Behaviour Understanding Group (IBUG)</a> at <a href="https://www.imperial.ac.uk/">Imperial College London (ICL)</a>, supervised by <a href="https://wp.doc.ic.ac.uk/szafeiri/">Stefanos Zafeiriou</a> and funded by the <a href="http://www.imperial.ac.uk/study/pg/fees-and-funding/scholarships/presidents-phd-scholarships/">Imperial President's PhD Scholarships</a>. I am in the project of <a href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N007743/1">EPSRC FACER2VM</a> (Face Matching for Automatic Identity Retrieval, Recognition, Verification and Management). My research topic is face analysis and I have particular interests in real-time visual localization (face, human and object).<br><br>

        Before coming to IBUG, I obtained my master and bachelor degrees from <a href="http://www.nuist.edu.cn/newindex/index.htm">Nanjing Universify of Information Science and Technology</a>. During my master study, I was with the <a href="http://bdat.nuist.edu.cn/">BDAT Lab</a> under the supervision of Prof. <a href="https://scholar.google.com/citations?user=FgFUwXAAAAAJ">Qingshan Liu</a>, and I worked closely with Prof. <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ">Dacheng Tao</a>.
      </div>
    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>

          <li> 2017-07-17:
						In the last three years, I have collected <alert>20/43 yellow bars</alert> (<a href="http://image-net.org/challenges/LSVRC/2017/results">10 in 2017</a>, <a href="http://image-net.org/challenges/LSVRC/2016/results">5 in 2016</a> and <a href="http://image-net.org/challenges/LSVRC/2015/results">5 in 2015</a>) from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). <a href="http://image-net.org/">ImageNet Challenge</a> is the most prestigious competition commonly known as the Olympics of computer vision.
          </li>

        </ul>
      </div>
    </div>
  </div>

	<div style="clear: both;">
		<div class="section">
			<h2>GPU Donations</h2>
			<div class="paper">
        <alert> Five NVIDIA GeForce GTX 1080 Ti donation from Kingsoft Cloud</alert><br>
        <alert> One NVIDIA GeForce GTX TITAN Xp donation from NVIDIA GPU Grant Program 2017</alert><br>
        <alert> One NVIDIA GeForce GTX TITAN Xp award from ActivityNet 2017 </alert><br>
			  <alert> Two NVIDIA GeForce GTX 1080 Ti funded by Imperial President's PhD Scholarships 2017 </alert><br>
				<alert> Two NVIDIA GeForce GTX 1080 funded by Imperial President's PhD Scholarships 2016 </alert><br>
				<alert> One NVIDIA GeForce GTX TITAN X award from ILSVRC 2015 </alert><br>
				<alert> ***      Any kind of GPU donation is welcome!      *** </alert>
			</div>
		</div>
	</div>

	<div style="clear: both;">
		<div class="section">
			<h2 id="confpapers">Academic Presentations</h2>
			<div class="paper">

			CVPR 2017, Beyond ImageNet Large Scale Visual Recognition Challenge, <a href="http://image-net.org/challenges/beyond_ilsvrc"> Speed/Accuracy Trade-offs for Object Detection from Video</a> <br>

			CVPR 2017, Beyond ImageNet Large Scale Visual Recognition Challenge, <a href="http://image-net.org/challenges/beyond_ilsvrc"> BDAT Object Detection </a> <br>

			ECCV 2016, ImageNet and COCO Visual Recognition Challenges, <a href="http://image-net.org/challenges/ilsvrc+coco2016"> Efficient Object Detection From Videos</a> <br>

			ICCV 2015, ImageNet and MS COCO Visual Recognition Challenges, <a href="http://image-net.org/challenges/ilsvrc+mscoco2015"> Cascade Region Regression for Robust Object Detection</a>

			<div class="spanner"></div>
			</div>
		</div>
	</div>

	<div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Academic Competitions</h2>
      <div class="paper">
        <ul>
          <li>
          ActivityNet 2017: Untrimmed Video Classification, <strong>Rank 1st</strong>.
				  </li>
          <li>
					ILSVRC 2017: Object detection with provided/additional training data, <strong>Rank 1st</strong>.
				  </li>
					<li>
					ILSVRC 2017: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2017: Object detection/tracking from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016: Object detection from video with provided/additional training data, <strong>Rank 1st</strong>.
					</li>
					<li>
					ILSVRC 2016: Object detection/tracking from video with additional training data, <strong>Rank 1st</strong>.
					</li>
          <li>
          ILSVRC 2015: Object detection with additional training data, <strong>Rank 1st</strong>.
          </li>
					<li>
					ILSVRC 2015: Object classification with additional training data, <strong>Rank 1st</strong>.
				  </li>
          <li>
          ILSVRC 2015: Object detection from video with additional training data, <strong>Rank 1st</strong>.
          </li>
					<li>
					300-VW 2015: Face detection, alignment and tracking from videos, <strong>Rank 1st</strong>.
					</li>
					<li>
					300-W 2014: Face detection and alignment from images, <strong>Rank 1st (Academic)</strong>.
					</li>
        </ul>
        <div class="spanner"></div>
      </div>
    </div>
  </div>

  <div style="clear: both;">
		<div class="section">
			<h2>Competition Organization</h2>
			<div class="paper">
			ICCV 2017,
			<a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">1st 3D Face Tracking in-the-wild Competition</a> <br>
			CVPR 2017,
			<a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">2nd Facial Landmark Localisation Competition - The Menpo BenchMark</a>
			</div>
		</div>
	</div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Datasets</h2>

      <div class="paper" id="menpo_ICCVW17"><img class="paper" src="./resources/papericon/menpo3D.png" title="The 3D Menpo Facial Landmark Tracking Challenge">
        <div> <strong>The 3D Menpo Facial Landmark Tracking Challenge</strong><br>
          S. Zafeiriou, G. Chrysos, A. Roussos, E. Ververas, <strong>J. Deng</strong> and G. Trigeorgis<br>
          in ICCVW, 2017. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Zafeiriou_3d_tracking_challenge_ICCV_2017_paper">Paper</a> ]
          [ <a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">Project Page</a>] <br>
          <alert>Full pose 3D face alignment dataset (84 landmarks)</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="menpo_CVPRW17"><img class="paper" src="./resources/papericon/menpobenchmark.png" title="The Menpo Facial Landmark Localisation Challenge: A step closer to the solution">
        <div> <strong>The Menpo Facial Landmark Localisation Challenge: A step closer to the solution</strong><br>
          S. Zafeiriou, G. Trigeorgis, G. Chrysos, <strong>J. Deng</strong> and J. Shen <br>
          in CVPRW, 2017. <br>
					[ <a href="http://jiankangdeng.github.io/resources/paper/Zafeiriou_The_Menpo_Facial_CVPR_2017_paper.pdf">Paper</a> ]
					[ <a href="https://ibug.doc.ic.ac.uk/resources/2nd-facial-landmark-tracking-competition-menpo-ben/">Project Page</a>] <br>
          <alert>Multi-view 2D face alignment dataset <br> (68 landmarks for semi-frontal faces and 39 landmarks for profile faces)</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="agedb_CVPRW17"><img class="paper" src="./resources/papericon/agedb.png" title="AgeDB: the first manually collected, in-the-wild age database">
        <div> <strong>AgeDB: the first manually collected, in-the-wild age database</strong><br>
          S. Moschoglou, A. Papaioannou, C. Sagonas, <strong>J. Deng</strong>, I. Kotsia, and S. Zafeiriou<br>
          in CVPRW, 2017. <br>
					[ <a href="http://jiankangdeng.github.io/resources/paper/agedb.pdf">Paper</a> ]
					[ <a href="https://ibug.doc.ic.ac.uk/resources/agedb/">Project Page</a>] <br>
          <alert>Manually collected in-the-wild age dataset</alert>
        </div>
        <div class="spanner"></div>
      </div>

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Selected Works [ <a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Jiankang">DBLP</a> ] [ <a href="https://scholar.google.com/citations?user=Z_UoQFsAAAAJ">Google Scholar</a> ] </h2>

      <div class="paper" id="AAAI_2018"><img class="paper" src="./resources/papericon/RPCA.png" title="Informed Non-convex Robust Principal Component Analysis with Features">
        <div> <strong>Informed Non-convex Robust Principal Component Analysis with Features</strong><br>
          N. Xue, <strong>J. Deng</strong>, I. Panagakis and S. Zafeiriou<br>
          in AAAI (Oral), 2018. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Xue_RPCA_for_AAAI_2018_paper.pdf">Paper</a> ]<br>
          <alert>Non-convex RPCA</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="TIP_2018"><img class="paper" src="./resources/papericon/MHM.png" title="Joint multi-view face alignment in the wild">
        <div> <strong>Joint multi-view face alignment in the wild</strong><br>
          <strong>J. Deng</strong>, G. Trigeorgis, Y. Zhou, and S. Zafeiriou<br>
          in arxiv, 2017. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Deng_MHM_submit_TIP_2017_paper.pdf">Paper</a> ]
          [ <a href="https://www.youtube.com/watch?v=TQU4yYF7pyQ">300VW Demo Results</a>]<br>
          [ <a href="http://pan.baidu.com/s/1jH6qUO2">2D Face Alignment Image Data</a>]<br>
          <alert>Joint multi-view face alignment</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="marginal_CVPRW17"><img class="paper" src="./resources/papericon/marginal.png" title="Marginal Loss for Deep Face Recognition">
        <div> <strong>Marginal loss for deep face recognition</strong><br>
          <strong>J. Deng</strong>, Y.Zhou and S. Zafeiriou<br>
          in CVPRW, 2017. <br>
					[ <a href="http://jiankangdeng.github.io/resources/paper/Deng_Marginal_Loss_for_CVPR_2017_paper.pdf">Paper</a> ]
					[ <a href="http://jiankangdeng.github.io/resources/poster/cvprw17_poster_marginal_loss.pdf">Poster</a>]
          [ <a href="http://nvlpubs.nist.gov/nistpubs/ir/2017/NIST.IR.8197.pdf">NIST Report</a>] <br>
          [ <a href="http://pan.baidu.com/s/1hsIM3RY">AgeDB_v1</a>]<br>
          <alert>Simple setting of data, network and loss for deep face recognition</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <!--
      <div class="paper" id="TIP_2017"><img class="paper" src="./resources/papericon/adaptive.png" title="Adaptive cascade regression model for robust face alignment">
        <div> <strong>Adaptive cascade regression model for robust face alignment</strong><br>
          Q. Liu, <strong>J. Deng</strong>, J. Yang, G. Liu, and D. Tao<br>
          in IEEE Transactions on Image Processing (<strong>TIP</strong>), 2017. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Liu_adaptive_for_TIP_2017_paper.pdf">Paper</a> ]<br>
          <alert>Adaptive shape constraint for face alignment under occlusion</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="TIP_2016"><img class="paper" src="./resources/papericon/dualsparse.png" title="Dual sparse constrained cascade regression for robust face alignment">
        <div> <strong>Dual sparse constrained cascade regression for robust face alignment</strong><br>
          Q. Liu, <strong>J. Deng</strong>, and D. Tao<br>
          in IEEE Transactions on Image Processing (<strong>TIP</strong>), 2016. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Liu_dualsparse_for_TIP_2016_paper.pdf">Paper</a> ]<br>
          <alert>Model compression by sparse constraint</alert>
        </div>
        <div class="spanner"></div>
      </div>
      -->

      <div class="paper" id="300VW_ICCVW15"><img class="paper" src="./resources/papericon/300VW.png" title="Facial shape tracking via spatio-temporal cascade shape regression">
        <div>
          <strong>Facial shape tracking via spatio-temporal cascade shape regression</strong><br>
          J. Yang, <strong>J. Deng</strong>, K. Zhang and Q. Liu<br>
          in ICCVW, 2015. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Yang_Facial_Shape_Tracking_ICCV_2015_paper.pdf">Paper</a> ]
          [ <a href="http://jiankangdeng.github.io/resources/video/300VW_mobile_binaryFeat.avi">Android Demo Binary Feature</a>]<br>
          [ <a href="http://pan.baidu.com/s/1slWHn0T">300VW Frame-wise Results</a>]<br>
          <!--[ <a href="http://pan.baidu.com/s/1eSeUXCQ">Lip Reading in the Wild (LRW) Dtaset Frame-wise Results</a>]<br>-->
          <alert>Winner of 300VW (Face detection, alignment and tracking from videos)</alert>
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="300W_IVC2014"><img class="paper" src="./resources/papericon/300W.png" title="M3CSR: multi-view, multi-scale and multi-component cascade shape regression">
        <div> <strong>M3CSR: multi-view, multi-scale and multi-component cascade shape regression</strong><br>
          <strong>J. Deng</strong>, J. Yang, Q. Liu and D. Tao<br>
          in Image Vision Computing, 2014. <br>
          [ <a href="http://jiankangdeng.github.io/resources/paper/Deng_M3CSR_for_IVC_2016_paper.pdf">Paper</a> ]
          [ <a href="http://jiankangdeng.github.io/resources/video/300W_mobile_HoG.avi">Android Demo HoG</a>]
          [ <a href="http://jiankangdeng.github.io/resources/video/300W_HOG_vs_binaryFeat.avi">Demo HoG vs Binary Feature</a>]<br>
          <alert>Academic winner of 300W (Face detection and alignment from images)</alert>
        </div>
        <div class="spanner"></div>
      </div>

    </div>
  </div>

	<div style="clear: both;">
		<div class="section">
			<h2>Intenships</h2>
			<div class="paper">

	    2015.03 - 2015.08: <strong>MSRA</strong>, Clothing Detection and Analysis. Work with <a href="https://scholar.google.com/citations?user=AUhj438AAAAJ">Shaoqing Ren</a>, <a href="https://scholar.google.com/citations?user=8qSLKUEAAAAJ">Yichen Wei</a> and <a href="https://scholar.google.com/citations?user=ALVSZAYAAAAJ">Jian Sun</a> in Visual Computing Group of Microsoft Research Asia.<br>

	    2013.12 - 2014.05: <strong>Baidu IDL</strong>, Face Detection and Analysis. Work with <a href="https://scholar.google.com/citations?user=rEYarG0AAAAJ">Junjie Yan</a>, <a href="https://scholar.google.com/citations?user=VUeFRoEAAAAJ">Yafeng Deng</a>, and <a href="https://scholar.google.com/citations?user=IyyEKyIAAAAJ">Chang Huang</a> in Baidu Institute of Deep Learning.

			</div>
		</div>
	</div>

  <div style="clear: both;">
    <div class="section">
      <h2>Friends</h2>
      <div class="paper">
        <a href="http://www.cbsr.ia.ac.cn/users/jjyan/main.htm">Junjie Yan</a> (SenseTime);
        <a href="http://shaoqingren.com/">Shaoqing Ren</a> (Momenta);
				<a href="https://www.microsoft.com/en-us/research/people/doch/">Dong Chen</a> (MSRA);
				<a href="http://www.jifengdai.org/">Jifeng Dai</a> (MSRA);
				<a href="https://www.microsoft.com/en-us/research/people/yichenw/">Yichen Wei</a> (MSRA);
				<a href="http://kaihuazhang.net/">Kaihua Zhang</a> (NUIST);
        <a href="https://sites.google.com/site/guangcanliu/">Guangcan Liu</a> (NUIST);
				<a href="https://sites.google.com/site/xtyuan1980/">Xiaotong Yuan</a> (NUIST);
				<a href="hhttps://scholar.google.com/citations?user=s_8zWpgAAAAJ&hl=zh-CN">Yi Wu</a> (NAU).
      </div>
    </div>
  </div>

  <div style='width:300px;height:300px;margin:0 auto'>
    <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=zXvpjqVexnJ3x1uCs9nueVR1Jl0Op9j_BdHdXNqZj_Q"></script>
  </div>

  <div style="clear:both;">
    <p align="right">
      <font size="5">Last Updated on 20th Nov., 2017</font>
    </p>
    <p align="right">
      <font size="5">Published with <a href="https://pages.github.com/">GitHub Pages</a></font>
    </p>
  </div>
</body>
</html>
